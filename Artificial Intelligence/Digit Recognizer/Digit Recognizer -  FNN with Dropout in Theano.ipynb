{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer - Feed Foward Net w/ Dropout in Theano\n",
    "\n",
    "**Kyu Cho**  \n",
    "**11/1/2016**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "- Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. \n",
    "- Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.  \n",
    "  \n",
    "- The training data set, (train.csv), has 785 columns. \n",
    "- The first column, called \"label\", is the digit that was drawn by the user. \n",
    "- The rest of the columns contain the pixel-values of the associated image.  \n",
    "  \n",
    "- Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. \n",
    "- To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. \n",
    "- Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n",
    "    + For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
    "\n",
    "000 001 002 003 ... 026 027  \n",
    "028 029 030 031 ... 054 055  \n",
    "056 057 058 059 ... 082 083  \n",
    " |   |   |   |  ...  |   |  \n",
    "728 729 730 731 ... 754 755  \n",
    "756 757 758 759 ... 782 783   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feed Forward\n",
    "2. Drop out\n",
    "3. Nesterov and RMSprop\n",
    "4. Ragulization\n",
    "5. Learning Rate\n",
    "6. Batch Gradient\n",
    "7. Normalization or PCA\n",
    "8. Dynamic number of nodes foor each layer\n",
    "9. Dynamic number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. Load Data\n",
    "2. Split Data\n",
    "3. Function initialization\n",
    "4. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from util import get_normalized_data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def get_normalized_data():\n",
    "    print \"Reading in and transforming data...\"\n",
    "    df = pd.read_csv('../large_files/train.csv')\n",
    "    \n",
    "    data = df.as_matrix().astype(np.float32)\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    Y = data[:, 0]\n",
    "    X = data[:, 1:]\n",
    "    \n",
    "    mu = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    np.place(std, std == 0, 1)\n",
    "    X = (X - mu) / std # normalize the data\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, M1, M2, an_id):\n",
    "        self.id = an_id\n",
    "        self.M1 = M1\n",
    "        self.M2 = M2\n",
    "        \n",
    "        # [init. a hidden layer]\n",
    "        W = np.random.randn(M1, M2) / np.sqrt(M1 + M2)\n",
    "        b = np.zeros(M2)\n",
    "        self.W = theano.shared(W, 'W_%s' % self.id)\n",
    "        self.b = theano.shared(b, 'b_%s' % self.id)\n",
    "        \n",
    "        # save params.\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        return T.nnet.relu(X.dot(self.W) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ANN(object):\n",
    "    def __init__(self, hidden_layer_sizes, p_keep):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.dropout_rates = p_keep # list of probabilities for each layer\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=10e-7, mu=0.99, decay=0.999, epochs=10, batch_sz=100, show_fig=False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X = X.astype(np.float32) # for GPU\n",
    "        Y = Y.astype(np.int32) # for GPU\n",
    "        \n",
    "        # split data set\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "    \n",
    "        # theano randomly generate number when it passes through it\n",
    "        self.rng = RandomStreams() \n",
    "    \n",
    "        # first hidden layer -> hidden layers -> last hidden layer\n",
    "        # [init. hidden layers]\n",
    "        N, D = X.shape\n",
    "        M1 = D  # save dim(784) for the first hidden layer\n",
    "\n",
    "        self.hidden_layers = []\n",
    "        count = 0\n",
    "        for M2 in self.hidden_layer_sizes: # [500, 300]\n",
    "            h = HiddenLayer(M1, M2, count) # (784, 500, 1), (500, 300, 2)\n",
    "            self.hidden_layers.append(h)\n",
    "            M1 = M2\n",
    "            count += 1\n",
    "\n",
    "        # [init. last layer]\n",
    "        K = len(set(Y)) # output length\n",
    "        W = np.random.randn(M1, K) / np.sqrt(M1 + K)\n",
    "        b = np.zeros(K)\n",
    "        self.W = theano.shared(W, 'W_logreg')\n",
    "        self.b = theano.shared(b, 'b_logreg')\n",
    "        \n",
    "        # save params from last param to first param(opposite) for backprob.\n",
    "        \n",
    "        # append previous hidden layers in to the last layer\n",
    "        # [(300, 10), (10,), (784, 500), (500,), (500, 300), (300,)]\n",
    "        self.params = [self.W, self.b]\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += h.params # self.params.extend(h.params)\n",
    "        \n",
    "     \n",
    "        \n",
    "        ################################ Theano Area Start ################################\n",
    "        # create theano var.\n",
    "        thX = T.matrix('X')\n",
    "        thY = T.ivector('Y')\n",
    "        \n",
    "        # define forward function w/ dropout \n",
    "        pY_train = self.forward_train(thX)\n",
    "        \n",
    "        # define cost function\n",
    "        # list [i, j] = return single index matching 'value'\n",
    "        # list[is, js] = return multiple index matching 'list'\n",
    "        cost = -T.mean(T.log(pY_train[T.arange(thY.shape[0]), thY]))\n",
    "        grads = T.grad(cost, self.params)\n",
    "        # init. empty value for all nodes for momentum, p = [wieght matrix, baies vector]\n",
    "        dparams = [theano.shared(np.zeros(p.get_value().shape)) for p in self.params]\n",
    "        # init. empty value for all nodes for rmsprop, p = [wieght matrix, baies vector]\n",
    "        cache = [theano.shared(np.zeros(p.get_value().shape)) for p in self.params]\n",
    "\n",
    "        updates = [\n",
    "            (c, decay*c + (1-decay)*g*g) for p, c, g in zip(self.params, cache, grads)\n",
    "        ] + [\n",
    "            (p, p + mu*dp - learning_rate*g/T.sqrt(c + 10e-10)) for p, c, dp, g in zip(self.params, cache, dparams, grads)\n",
    "        ] + [\n",
    "            (dp, mu*dp - learning_rate*g/T.sqrt(c + 10e-10)) for p, c, dp, g in zip(self.params, cache, dparams, grads)\n",
    "        ]\n",
    "        \n",
    "        train_op = theano.function(\n",
    "            inputs = [thX, thY],\n",
    "            updates = updates\n",
    "        )\n",
    "\n",
    "        ################################ Theano Area End ################################\n",
    "        # for evaluation and prediction\n",
    "        pY_predict = self.forward_predict(thX)\n",
    "        cost_predict = -T.mean(T.log(pY_predict[T.arange(thY.shape[0]), thY]))\n",
    "        prediction = self.predict(thX)\n",
    "        cost_predict_op = theano.function(\n",
    "            inputs = [thX, thY], \n",
    "            outputs = [cost_predict, prediction]\n",
    "        )\n",
    "\n",
    "        n_batches = N / batch_sz\n",
    "        costs = []\n",
    "        for i in xrange(epochs):\n",
    "            X, Y = shuffle(X, Y)\n",
    "            \n",
    "            for j in xrange(n_batches):\n",
    "                Xbatch = X[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "                Ybatch = Y[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "\n",
    "                train_op(Xbatch, Ybatch)\n",
    "\n",
    "                if j % 20 == 0:\n",
    "                    c, p = cost_predict_op(Xvalid, Yvalid)\n",
    "                    costs.append(c)\n",
    "                    e = error_rate(Yvalid, p)\n",
    "                    print \"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c, \"error rate:\", e\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "        \n",
    "    # X = data (theano value)\n",
    "    def forward_train(self, X):\n",
    "        Z = X\n",
    "        for h, p in zip(self.hidden_layers, self.dropout_rates[:-1]): # [500, .8], [300, .5]\n",
    "            # init. random 0,1 for all nodes for dropout masking\n",
    "            mask = self.rng.binomial(n=1, p=p, size=Z.shape) # n=1 (0 or 1)\n",
    "            Z = mask * Z\n",
    "            Z = h.forward(Z)\n",
    "        \n",
    "        # ?? probability has list of values\n",
    "        mask = self.rng.binomial(n=1, p=self.dropout_rates[-1], size=Z.shape)\n",
    "        Z = mask * Z\n",
    "        return T.nnet.softmax(Z.dot(self.W) + self.b)\n",
    "\n",
    "    def forward_predict(self, X):\n",
    "        Z = X\n",
    "        for h, p in zip(self.hidden_layers, self.dropout_rates[:-1]):\n",
    "            Z = h.forward(p * Z)\n",
    "        return T.nnet.softmax((self.dropout_rates[-1] * Z).dot(self.W) + self.b)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pY = self.forward_predict(X)\n",
    "        return T.argmax(pY, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimention of X (42000, 784)\n",
      "Dimention of Y (42000,)\n",
      "[ 5.  6.  2. ...,  3.  7.  4.]\n"
     ]
    }
   ],
   "source": [
    "def error_rate(p, t):\n",
    "    return np.mean(p != t)\n",
    "\n",
    "def relu(a):\n",
    "    return a * (a > 0)\n",
    "\n",
    "def main():\n",
    "#     X, Y = get_normalized_data()\n",
    "    print 'Dimention of X', X.shape\n",
    "    print 'Dimention of Y',Y.shape\n",
    "    print Y\n",
    "    # hidden layer size , drop out rate\n",
    "#     ann = ANN([500, 300], [0.8, 0.5, 0.5])\n",
    "#     ann.fit(X, Y, show_fig=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
