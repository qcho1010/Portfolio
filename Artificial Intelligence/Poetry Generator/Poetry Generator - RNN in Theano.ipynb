{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poetry Generator - RNN in Theano\n",
    "**Kyu Cho**  \n",
    "**11/13/16**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "- Popular method in deep learning  \n",
    "- Word embeddings or word vetors  \n",
    "- One-hot word vector of size $V$ becomes a smaller vector size $D$, $D < V$  \n",
    "-  Train models to do tasks like predict next word / surrounding words // sentiment analysis  \n",
    "-  Use word embeddings as input instead of one-hot vectors, but train the word embeddings as part of the model  \n",
    "- Result is meaningful word embeddings, allowing us to do arithmetic:  \n",
    "-  King - Man ~= Queen - Woman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the Word Embedding\n",
    "- Given: $W_e : V \\times D$ matrix ($V$ = vocab. size, $D$ = word vector dim.)  \n",
    "- Given: input sequence of word indexes (length $T$)  \n",
    "- Output: $T \\times D$ matrix containing a sequence of word vectors  \n",
    "-  Constrains:  Impossible to make the $T \\times D$ matrix the input to the neural network, because we want $W_e$ to be an updateable parameter.  \n",
    "- Input of $T$ integer is much smaller than input $T \\times D$ floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import string\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(None, string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_robert_frost():\n",
    "    word2idx = {'START': 0, 'END': 1}  # mapping dictionary\n",
    "    current_idx = 2\n",
    "    sentences = []  # converted sentences in integer values\n",
    "    \n",
    "    for line in open('robert_frost.txt'):\n",
    "        line = line.strip() # remove new lines\n",
    "    \n",
    "        if line:\n",
    "            tokens = remove_punctuation(line.lower()).split() # tokenize cleanned words\n",
    "            sentence = []\n",
    "        \n",
    "            for t in tokens:\n",
    "            \n",
    "                if t not in word2idx: # if dict. doesn't have the token\n",
    "                    word2idx[t] = current_idx # create new key as token with idx. value\n",
    "                    current_idx += 1 # increment the idx.  ex) max. idx. == total # of unique tokens \n",
    "                    \n",
    "                idx = word2idx[t] # save current token idx.\n",
    "                sentence.append(idx) # append current token idx to sentence list\n",
    "            sentences.append(sentence) # append converted sentense into sentenses\n",
    "\n",
    "    return sentences, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(Mi, Mo):\n",
    "    return np.random.randn(Mi, Mo) / np.sqrt(Mi + Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, D, M, V):\n",
    "        self.D = D # dimensionality of word embedding\n",
    "        self.M = M # hidden layer size\n",
    "        self.V = V # vocabulary size\n",
    "\n",
    "    def fit(self, X, learning_rate=10e-1, mu=0.99, reg=1.0, activation=T.tanh, epochs=500, show_fig=False):\n",
    "        N = len(X) # 15 - number of sentense\n",
    "        D = self.D # 30 - dimensionality of word embedding\n",
    "        M = self.M # 30 - no. of nodes in each layer size \n",
    "        V = self.V # 81 - no. of unique vocab.\n",
    "        self.f = activation\n",
    "\n",
    "        # initial weights\n",
    "        We = init_weight(V, D) # 81 x 30\n",
    "        Wx = init_weight(D, M) # 30 x 30\n",
    "        Wh = init_weight(M, M) # 30 x 30\n",
    "        bh = np.zeros(M) # 30\n",
    "        h0 = np.zeros(M) # 30\n",
    "        Wo = init_weight(M, V) # 30 x 81\n",
    "        bo = np.zeros(V) # 81\n",
    "\n",
    "        # make them theano shared\n",
    "        self.We = theano.shared(We)\n",
    "        self.Wx = theano.shared(Wx)\n",
    "        self.Wh = theano.shared(Wh)\n",
    "        self.bh = theano.shared(bh)\n",
    "        self.h0 = theano.shared(h0)\n",
    "        self.Wo = theano.shared(Wo)\n",
    "        self.bo = theano.shared(bo)\n",
    "        self.params = [self.We, self.Wx, self.Wh, self.bh, self.h0, self.Wo, self.bo]\n",
    "\n",
    "        thX = T.ivector('X')\n",
    "        Ei = self.We[thX] # We = 81 x 30, thx = list of row idx., each word has own weight in every layer M. (# of words in a sentense x layer size)\n",
    "        thY = T.ivector('Y')\n",
    "\n",
    "        # [START, w1, w2, ..., wn]\n",
    "        # sentence target:\n",
    "        # [w1,    w2, w3, ..., END]\n",
    "\n",
    "        def recurrence(x_t, h_t1): # x_t = single word weights for every layer dim = (30,)\n",
    "            h_t = self.f(x_t.dot(self.Wx) + h_t1.dot(self.Wh) + self.bh) # h_t = (30,) (weight for a single word in each layer)\n",
    "            y_t = T.flatten(T.nnet.softmax(h_t.dot(self.Wo) + self.bo), outdim=1) # y_t = (81,)\n",
    "            return h_t, y_t\n",
    "\n",
    "        # scan function only runs whenever Ei value is filled\n",
    "        [h, y], _ = theano.scan(\n",
    "            fn = recurrence,\n",
    "            outputs_info = [self.h0, None],\n",
    "            sequences = Ei, # no. of words x 30 (weight matrix), will go through every word\n",
    "            n_steps = Ei.shape[0],\n",
    "        )\n",
    "\n",
    "        py_x = y # (no. of words in each sentense, no. of total unique words)   ex) (8, 81)\n",
    "        prediction = T.argmax(py_x, axis=1) #  (argmax 1 returns idx. of max number in every row) \n",
    "\n",
    "        cost = -T.mean(T.log(py_x[T.arange(thY.shape[0]), thY])) #\n",
    "        grads = T.grad(cost, self.params)\n",
    "        dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "\n",
    "        updates = [\n",
    "            (p, p + mu*dp - learning_rate*g) for p, dp, g in zip(self.params, dparams, grads)\n",
    "        ] + [\n",
    "            (dp, mu*dp - learning_rate*g) for dp, g in zip(dparams, grads)\n",
    "        ]\n",
    "\n",
    "        self.predict_op = theano.function(\n",
    "            inputs = [thX], \n",
    "            outputs = prediction)\n",
    "        \n",
    "        self.train_op = theano.function(\n",
    "            inputs = [thX, thY],\n",
    "            outputs =[cost, prediction, h, y],\n",
    "            updates = updates\n",
    "        )\n",
    "    \n",
    "        costs = []\n",
    "        n_total = sum((len(sentence)+1) for sentence in X) # total length of sentenses\n",
    "    \n",
    "        for i in xrange(epochs):\n",
    "            X = shuffle(X)\n",
    "            n_correct = 0\n",
    "            cost = 0\n",
    "            \n",
    "            for j in xrange(N): # N = len(X) - number of sentense\n",
    "                # problem! many words --> END token are overrepresented\n",
    "                # result: generated lines will be very short\n",
    "\n",
    "                # set 0 to start and 1 to end\n",
    "                input_sequence = [0] + X[j]\n",
    "                output_sequence = X[j] + [1]\n",
    "\n",
    "                c, p, hout, rout = self.train_op(input_sequence, output_sequence)\n",
    "                cost += c\n",
    "#                 print \"p:\", p\n",
    "\n",
    "                for pj, xj in zip(p, output_sequence):\n",
    "                    if pj == xj:\n",
    "                        n_correct += 1\n",
    "\n",
    "            print \"y:\", rout.shape # (no. of words in each sentense, 1, no. of outputs)   ex) (8, 1, 81)\n",
    "            print \"i:\", i, \"cost:\", cost, \"correct rate:\", (float(n_correct)/n_total)\n",
    "            costs.append(cost)\n",
    "\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "            \n",
    "    def save(self, filename): \n",
    "        np.savez(filename, *[p.get_value() for p in self.params]) # save multiple arr. at once\n",
    "        \n",
    "    @staticmethod\n",
    "    def load(filename, activation):\n",
    "        # TODO: would prefer to save activation to file too\n",
    "        npz = np.load(filename)\n",
    "        We = npz['arr_0']\n",
    "        Wx = npz['arr_1']\n",
    "        Wh = npz['arr_2']\n",
    "        bh = npz['arr_3']\n",
    "        h0 = npz['arr_4']\n",
    "        Wo = npz['arr_5']\n",
    "        bo = npz['arr_6']\n",
    "        V, D = We.shape\n",
    "        _, M = Wx.shape\n",
    "        rnn = SimpleRNN(D, M, V)\n",
    "        rnn.set(We, Wx, Wh, bh, h0, Wo, bo, activation)\n",
    "        return rnn\n",
    "    \n",
    "    def set(self, We, Wx, Wh, bh, h0, Wo, bo, activation):\n",
    "        self.f = activation\n",
    "        self.We = theano.shared(We)\n",
    "        self.Wx = theano.shared(Wx)\n",
    "        self.Wh = theano.shared(Wh)\n",
    "        self.bh = theano.shared(bh)\n",
    "        self.h0 = theano.shared(h0)\n",
    "        self.Wo = theano.shared(Wo)\n",
    "        self.bo = theano.shared(bo)\n",
    "        self.params = [self.We, self.Wx, self.Wh, self.bh, self.h0, self.Wo, self.bo]\n",
    "\n",
    "        thX = T.ivector('X')\n",
    "        Ei = self.We[thX] # will be a TxD matrix\n",
    "        thY = T.ivector('Y')\n",
    "\n",
    "        def recurrence(x_t, h_t1): # x_t = single word weights for every layer dim = (30,)\n",
    "            h_t = self.f(x_t.dot(self.Wx) + h_t1.dot(self.Wh) + self.bh) # h_t = (30,) (weight for a single word in each layer)\n",
    "            y_t = T.flatten(T.nnet.softmax(h_t.dot(self.Wo) + self.bo), outdim=1) # y_t = (81,)\n",
    "            return h_t, y_t\n",
    "\n",
    "        # scan function only runs whenever Ei value is filled\n",
    "        [h, y], _ = theano.scan(\n",
    "            fn = recurrence,\n",
    "            outputs_info = [self.h0, None],\n",
    "            sequences = Ei, # no. of words x 30 (weight matrix), will go through every word\n",
    "            n_steps = Ei.shape[0],\n",
    "        )\n",
    "\n",
    "        py_x = y # (no. of words in each sentense, no. of total unique words)   ex) (8, 81)\n",
    "        prediction = T.argmax(py_x, axis=1) #  (argmax 1 returns idx. of max number in every row) \n",
    "\n",
    "        self.predict_op = theano.function(\n",
    "            inputs = [thX],\n",
    "            outputs = prediction,\n",
    "            allow_input_downcast = True,\n",
    "        )\n",
    "        \n",
    "    # pi = word counts distribution,  word2idx = dict. ex) {START:0, ... }\n",
    "    def generate(self, pi, word2idx):\n",
    "        # convert word2idx -> idx2word ex) {0:START, ...\n",
    "        idx2word = {v:k for k,v in word2idx.iteritems()} # iteritems() to iterate word2idx dict.\n",
    "        V = len(pi)\n",
    "        \n",
    "        # pi is words distribution, X will have higher change to randomly generate the most used words\n",
    "        # why random? because using the START symbol will always yield the same first word\n",
    "        X = [np.random.choice(V, p=pi)] # random.choice(5, 3, p=non-uniform distribut.) -> array([0, 3, 4])\n",
    "        print idx2word[X[0]] ,# print the first randomly selected word\n",
    "\n",
    "        # generate 6 lines at a time\n",
    "        n_lines = 0\n",
    "        while n_lines < 6:\n",
    "            P = self.predict_op(X)[-1]# [-1] converts arr. into integer\n",
    "            X += [P]# append to current sentense\n",
    "            \n",
    "            if P > 1: # if it's not a real word, not start/end token\n",
    "                word = idx2word[P]  # map the word\n",
    "                print word,\n",
    "            elif P == 1:  # if it's end token\n",
    "                # end token\n",
    "                n_lines += 1\n",
    "                print ''\n",
    "                if n_lines < 6:\n",
    "                    X = [ np.random.choice(V, p=pi) ] # reset to start of line\n",
    "                    print idx2word[X[0]],\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_poetry():\n",
    "    sentences, word2idx = get_robert_frost()\n",
    "    rnn = SimpleRNN.load('RNN_D30_M30_epochs2000_relu.npz', T.nnet.relu)\n",
    "\n",
    "    # determine initial state distribution for starting sentences\n",
    "    V = len(word2idx)\n",
    "    pi = np.zeros(V)\n",
    "    \n",
    "    # create the first word distribution\n",
    "    for sentence in sentences: \n",
    "        # get the first word of the sentense\n",
    "        # increment the count at the location in pi distribution\n",
    "        pi[sentence[0]] += 1\n",
    "        \n",
    "    pi /= pi.sum() # convert every count as percentages\n",
    "    \n",
    "\n",
    "    rnn.generate(pi, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_poetry():\n",
    "    sentences, word2idx = get_robert_frost()\n",
    "    rnn = SimpleRNN(30, 30, len(word2idx)) # len(word2idx) = no. of unique words ex) 81\n",
    "#     rnn.fit(sentences, learning_rate=10e-5, show_fig=True, activation=T.nnet.relu, epochs=200)\n",
    "#     rnn.save('RNN_D30_M30_epochs2000_relu.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man \n",
      "farmers and then and reconciled \n",
      "he cant do \n",
      "the door of the book of the ground \n",
      "its old tower clock \n",
      "and then and both \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_poetry()\n",
    "    generate_poetry()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
