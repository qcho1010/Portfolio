{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poetry Generator2 - RRNN in Theano\n",
    "**Kyu Cho**  \n",
    "**12/9/16**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "- Popular method in deep learning  \n",
    "- Word embeddings or word vetors  \n",
    "- One-hot word vector of size $V$ becomes a smaller vector size $D$, $D < V$  \n",
    "-  Train models to do tasks like predict next word / surrounding words // sentiment analysis  \n",
    "-  Use word embeddings as input instead of one-hot vectors, but train the word embeddings as part of the model  \n",
    "- Result is meaningful word embeddings, allowing us to do arithmetic:  \n",
    "-  King - Man ~= Queen - Woman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing the Word Embedding\n",
    "- Given: $W_e : V \\times D$ matrix ($V$ = vocab. size, $D$ = word vector dim.)  \n",
    "- Given: input sequence of word indexes (length $T$)  \n",
    "- Output: $T \\times D$ matrix containing a sequence of word vectors  \n",
    "-  Constrains:  Impossible to make the $T \\times D$ matrix the input to the neural network, because we want $W_e$ to be an updateable parameter.  \n",
    "- Input of $T$ integer is much smaller than input $T \\times D$ floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import string\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(None, string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_robert_frost():\n",
    "    word2idx = {'START': 0, 'END': 1}  # mapping dictionary\n",
    "    current_idx = 2\n",
    "    sentences = []  # converted sentences in integer values\n",
    "    \n",
    "    for line in open('robert_frost.txt'):\n",
    "        line = line.strip() # remove new lines\n",
    "    \n",
    "        if line:\n",
    "            tokens = remove_punctuation(line.lower()).split() # tokenize cleanned words\n",
    "            sentence = []\n",
    "        \n",
    "            for t in tokens:\n",
    "            \n",
    "                if t not in word2idx: # if dict. doesn't have the token\n",
    "                    word2idx[t] = current_idx # create new key as token with idx. value\n",
    "                    current_idx += 1 # increment the idx.  ex) max. idx. == total # of unique tokens \n",
    "                    \n",
    "                idx = word2idx[t] # save current token idx.\n",
    "                sentence.append(idx) # append current token idx to sentence list\n",
    "            sentences.append(sentence) # append converted sentense into sentenses\n",
    "\n",
    "    return sentences, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(Mi, Mo):\n",
    "    return np.random.randn(Mi, Mo) / np.sqrt(Mi + Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, D, M, V):\n",
    "        self.D = D # dimensionality of word embedding\n",
    "        self.M = M # hidden layer size\n",
    "        self.V = V # vocabulary size\n",
    "\n",
    "    def fit(self, X, learning_rate=10e-1, mu=0.99, reg=1.0, activation=T.tanh, epochs=500, show_fig=False):\n",
    "        N = len(X) # 1500 - number of sentense\n",
    "        D = self.D # 50 - dimensionality of word embedding\n",
    "        M = self.M # 50 - no. of nodes in each layer size \n",
    "        V = self.V # 2200 - no. of unique vocab for outcome\n",
    "\n",
    "        # initial weights\n",
    "        We = init_weight(V, D) # 2200 x 50\n",
    "        Wx = init_weight(D, M) # 50 X 50\n",
    "        Wh = init_weight(M, M) # 50 x 50\n",
    "        bh = np.zeros(M)\n",
    "        h0 = np.zeros(M)\n",
    "        \n",
    "        # z  = np.ones(M)\n",
    "        Wxz = init_weight(D, M)\n",
    "        Whz = init_weight(M, M)\n",
    "        bz  = np.zeros(M)\n",
    "        Wo = init_weight(M, V)\n",
    "        bo = np.zeros(V)\n",
    "        \n",
    "        thX, thY, py_x, prediction = self.set(We, Wx, Wh, bh, h0, Wxz, Whz, bz, Wo, bo, activation)\n",
    "\n",
    "        lr = T.scalar('lr')\n",
    "\n",
    "        cost = -T.mean(T.log(py_x[T.arange(thY.shape[0]), thY]))\n",
    "        grads = T.grad(cost, self.params)\n",
    "        dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "        \n",
    "        updates = [\n",
    "            (p, p + mu*dp - lr*g) for p, dp, g in zip(self.params, dparams, grads)\n",
    "        ] + [\n",
    "            (dp, mu*dp - lr*g) for dp, g in zip(dparams, grads)\n",
    "        ]\n",
    "\n",
    "        self.predict_op = theano.function(\n",
    "            inputs = [thX], \n",
    "            outputs = prediction\n",
    "        )\n",
    "        self.train_op = theano.function(\n",
    "            inputs = [thX, thY, lr],\n",
    "            outputs = [cost, prediction],\n",
    "            updates = updates\n",
    "        )\n",
    "\n",
    "        costs = []\n",
    "        for i in xrange(epochs):\n",
    "            X = shuffle(X)\n",
    "            n_correct = 0\n",
    "            cost = 0\n",
    "            n_total = 0\n",
    "            for j in xrange(N):\n",
    "                if np.random.random() < 0.1:\n",
    "                    input_sequence = [0] + X[j]\n",
    "                    output_sequence = X[j] + [1]\n",
    "                else:\n",
    "                    input_sequence = [0] + X[j][:-1]\n",
    "                    output_sequence = X[j]\n",
    "                n_total += len(output_sequence)\n",
    "\n",
    "                # we set 0 to start and 1 to end\n",
    "                c, p = self.train_op(input_sequence, output_sequence, learning_rate)\n",
    "                # print \"p:\", p\n",
    "                cost += c\n",
    "                # print \"j:\", j, \"c:\", c/len(X[j]+1)\n",
    "                for pj, xj in zip(p, output_sequence):\n",
    "                    if pj == xj:\n",
    "                        n_correct += 1\n",
    "            print \"i:\", i, \"cost:\", cost, \"correct rate:\", (float(n_correct)/n_total)\n",
    "            if (i + 1) % 500 == 0:\n",
    "                learning_rate /= 2\n",
    "            costs.append(cost)\n",
    "\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "            \n",
    "    def save(self, filename): \n",
    "        np.savez(filename, *[p.get_value() for p in self.params]) # save multiple arr. at once\n",
    "        \n",
    "    @staticmethod\n",
    "    def load(filename, activation):\n",
    "        # TODO: would prefer to save activation to file too\n",
    "        npz = np.load(filename)\n",
    "        We = npz['arr_0']\n",
    "        Wx = npz['arr_1']\n",
    "        Wh = npz['arr_2']\n",
    "        bh = npz['arr_3']\n",
    "        h0 = npz['arr_4']\n",
    "        Wxz = npz['arr_5']\n",
    "        Whz = npz['arr_6']\n",
    "        bz = npz['arr_7']\n",
    "        Wo = npz['arr_8']\n",
    "        bo = npz['arr_9']\n",
    "        V, D = We.shape\n",
    "        _, M = Wx.shape\n",
    "        rnn = SimpleRNN(D, M, V)\n",
    "        rnn.set(We, Wx, Wh, bh, h0, Wxz, Whz, bz, Wo, bo, activation)\n",
    "        return rnn\n",
    "    \n",
    "    def set(self, We, Wx, Wh, bh, h0, Wxz, Whz, bz, Wo, bo, activation):\n",
    "        self.f = activation\n",
    "\n",
    "        # redundant - see how you can improve it\n",
    "        self.We = theano.shared(We)\n",
    "        self.Wx = theano.shared(Wx)\n",
    "        self.Wh = theano.shared(Wh)\n",
    "        self.bh = theano.shared(bh)\n",
    "        self.h0 = theano.shared(h0)\n",
    "        self.Wxz = theano.shared(Wxz)\n",
    "        self.Whz = theano.shared(Whz)\n",
    "        self.bz = theano.shared(bz)\n",
    "        self.Wo = theano.shared(Wo)\n",
    "        self.bo = theano.shared(bo)\n",
    "        self.params = [self.We, self.Wx, self.Wh, self.bh, self.h0, self.Wxz, self.Whz, self.bz, self.Wo, self.bo]\n",
    "\n",
    "        \n",
    "        thX = T.ivector('X')\n",
    "        Ei = self.We[thX] # We = 2300 x 50, thx = list of row idx., each word has own weight in every layer M. (# of words in a sentense x layer size)\n",
    "        thY = T.ivector('Y')\n",
    "\n",
    "        def recurrence(x_t, h_t1): # x_t = single word weights for every layer dim = (50,)\n",
    "            # h_t = (50,) (weight for a single word in each layer)\n",
    "            # y_t = (81,)\n",
    "            hhat_t = self.f(x_t.dot(self.Wx) + h_t1.dot(self.Wh) + self.bh) \n",
    "            z_t = T.nnet.sigmoid(x_t.dot(self.Wxz) + h_t1.dot(self.Whz) + self.bz)\n",
    "            h_t = (1 - z_t) * h_t1 + z_t * hhat_t\n",
    "            y_t = T.nnet.softmax(h_t.dot(self.Wo) + self.bo)\n",
    "            return h_t, y_t\n",
    "        \n",
    "        [h, y], _ = theano.scan(\n",
    "            fn=recurrence,\n",
    "            outputs_info=[self.h0, None],\n",
    "            sequences=Ei, # no. of words x 50 (weight matrix), will go through every word\n",
    "            n_steps=Ei.shape[0],\n",
    "        )\n",
    "\n",
    "        py_x = y[:, 0, :]  # (no. of words in each sentense, no. of total unique words)   ex) (8, 2200)\n",
    "        prediction = T.argmax(py_x, axis=1)\n",
    "        \n",
    "        self.predict_op = theano.function(\n",
    "            inputs=[thX],\n",
    "            outputs=[py_x, prediction],\n",
    "            allow_input_downcast=True,\n",
    "        )\n",
    "        \n",
    "        return thX, thY, py_x, prediction\n",
    "\n",
    "\n",
    "        \n",
    "    # pi = word counts distribution,  word2idx = dict. ex) {START:0, ... }\n",
    "    def generate(self, word2idx):\n",
    "        # convert word2idx -> idx2word ex) {0:START, ...\n",
    "        idx2word = {v:k for k,v in word2idx.iteritems()} # iteritems() to iterate word2idx dict.\n",
    "        V = len(word2idx)\n",
    "   \n",
    "        # pi is words distribution, X will have higher change to randomly generate the most used words\n",
    "        # why random? because using the START symbol will always yield the same first word\n",
    "        X = [0] # start token\n",
    "        # generate 6 lines at a time\n",
    "        n_lines = 0\n",
    "        while n_lines < 6:\n",
    "            PY_X, _ = self.predict_op(X)\n",
    "            PY_X = PY_X[-1].flatten()\n",
    "            P = [ np.random.choice(V, p=PY_X)] # anything from 0 to V with prob. of py_x\n",
    "            X = np.concatenate([X, P]) # append to the sequence\n",
    "            # print \"P.shape:\", P.shape, \"P:\", P\n",
    "            P = P[-1] # just grab the most recent prediction\n",
    "            if P > 1: # if it's not a real word, not start/end token\n",
    "                word = idx2word[P]  # map the word\n",
    "                print word,\n",
    "            elif P == 1:  # if it's end token\n",
    "                # end token\n",
    "                n_lines += 1\n",
    "                X = [0]\n",
    "                print ''\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_poetry():\n",
    "    sentences, word2idx = get_robert_frost()\n",
    "    rnn = SimpleRNN.load('RRNN_D50_M50_epochs200_relu.npz', T.nnet.relu)\n",
    "    rnn.generate(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_poetry():\n",
    "    # students: tanh didn't work but you should try it\n",
    "    sentences, word2idx = get_robert_frost()\n",
    "    rnn = SimpleRNN(50, 50, len(word2idx))\n",
    "#     print len(word2idx)\n",
    "    rnn.fit(sentences, learning_rate=10e-5, show_fig=True, activation=T.nnet.relu, epochs=200)\n",
    "    rnn.save('RRNN_D50_M50_epochs200_relu.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if god might prove the waterfall began girdle to say \n",
      "but you get as upstairs and this covered to what that \n",
      "mother were ago the chance for \n",
      "proclaimed i would heard my thought the say from though \n",
      "he wont takes \n",
      "whats not geese to harm between ourselves you do off it died though that im after than \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     train_poetry()\n",
    "    generate_poetry()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
